{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to cross-lingual word-embeddings at Wikimania 2019\n",
    "\n",
    "* In this tutorial we will learn how to work with cross-lingual word embeddings. \n",
    "* This code is based on the repository shared by [Smith et al](https://github.com/Babylonpartners/fastText_multilingual)\n",
    "* You can see applications of these code on the Wikipedia [Sections](https://github.com/digitalTranshumant/wmf-interlanguage) and [Template parameters](https://github.com/digitalTranshumant/templatesAlignment) alignments.\n",
    "\n",
    "* You will need to download\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Config \n",
    "## Add here your folders and languages\n",
    "import fastText\n",
    "from scipy.spatial import distance\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "lang1 = 'en'\n",
    "lang2 = 'es'\n",
    "langs =[lang1,lang2]\n",
    "pathVectors = 'vectors/' \n",
    "pathAlignment = 'wikiAlignments/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download fasttext models\n",
    "\n",
    "* This script download the fasttext pre-trained models in the languages listed langs variable.\n",
    "* This process **can take long time**.\n",
    "* Note that **each model file is around 8G** , and later you will need to unzip those models, using around 15G per model in total.\n",
    "* Comment (add # prefix)the first line in the next cell to download the models. If you already have the models you in your folder, you can skip this step. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMMENT  HERE TO RUN THIS CELL\n",
    "\n",
    "!mkdir {pathVectors}\n",
    "for l in lang:\n",
    "    print(l)\n",
    "    !wget -P vectors/ {'https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.%s.zip' % l}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the models, this can take few minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = {}\n",
    "for lang in langs:\n",
    "    model[lang] = fastText.load_model('%s/wiki.%s.bin' % (pathVectors,lang))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download transformation Matrices\n",
    "* Note that his repository already contains transformation from 'en' to 'es'\n",
    "* This alignments are generated using [this code](https://analytics.wikimedia.org/datasets/one-off/dsaez/)\n",
    "* If you need to a pair of languages that is not contained here, please contact us, or use the pre-trained [provided here](https://github.com/Babylonpartners/fastText_multilingual/tree/master/alignment_matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMMENT  HERE TO RUN THIS CELL\n",
    "!mkdir {pathAlignment} #comment here if the folter already exists\n",
    "for l in lang:\n",
    "    print(l)\n",
    "    !wget -P {pathAlignment} {'apply_in_%s_to_%s.txt ' % (lang1,lang2)}\n",
    "    !wget -P {pathAlignment} {'apply_in_%s_to_%s.txt ' % (lang2,lang1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = model['es'].get_sentence_vector('perro')\n",
    "v2 = model['en'].get_sentence_vector('dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9362809884474315"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance.cosine(v1,v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function apply the transformation to a given vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_transform(vec, transform):\n",
    "        \"\"\"\n",
    "        Apply the given transformation to the vector space\n",
    "\n",
    "        Right-multiplies given transform with embeddings E:\n",
    "            E = E * transform\n",
    "\n",
    "        Transform can either be a string with a filename to a\n",
    "        text file containing a ndarray (compat. with np.loadtxt)\n",
    "        or a numpy ndarray.\n",
    "        \"\"\"\n",
    "        transmat = np.loadtxt(transform) if isinstance(transform, str) else transform\n",
    "        return np.matmul(vec, transmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align second language\n",
    "v2Aligned = apply_transform(v2,'%s/apply_in_%s_to_%s.txt' % (pathAlignment,'en','es') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2947399014085661"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance.cosine(v1,v2Aligned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subword information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using subword information with modified or misspelled words\n",
    "\n",
    "within the same language:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42508700188863213"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1 = model['en'].get_sentence_vector('excellent')\n",
    "v2 = model['en'].get_sentence_vector('excelent')\n",
    "distance.cosine(v1,v2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "or with cross-lingual aligned vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4684736133135168"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1 = model['es'].get_sentence_vector('perro1')\n",
    "v2 = model['en'].get_sentence_vector('dog')\n",
    "v2Aligned = apply_transform(v2,'%s/apply_in_%s_to_%s.txt' % (pathAlignment,'en','es') )\n",
    "distance.cosine(v1,v2Aligned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = model['es'].get_sentence_vector('Que tenga un bonito día señor')\n",
    "sentence2 = model['en'].get_sentence_vector('Have a nice day sir')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0227684068147005"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance.cosine(sentence1,sentence2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence2Aligned = apply_transform(v2,'%s/apply_in_%s_to_%s.txt' % (pathAlignment,'en','es') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6579386823118475"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance.cosine(sentence1,sentence2Aligned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aligning  sets of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load all transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en\n",
      "es\n"
     ]
    }
   ],
   "source": [
    "transmat = {}\n",
    "for lang in langs:\n",
    "    print(lang)\n",
    "    transmat[lang] = {}\n",
    "    for lang2 in langs:\n",
    "        if lang!=lang2:\n",
    "            transmat[lang][lang2] = np.loadtxt('%s/apply_in_%s_to_%s.txt' % (pathAlignment,lang2,lang))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = {}\n",
    "words[lang1] = ['cat','kitty','motocycle','car','dog','truck','geography','mountains','rivers','basketball','football']\n",
    "words[lang2] = ['gato','automóvil','perro','camión','geografía','montañas','rios','baloncesto','futbol']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMoreSimilar(wordLang1,setLang2,sourceLang,targetLang):\n",
    "    \"\"\"\n",
    "    Given a word in language 1 and set of words/sentences language 2\n",
    "    return \n",
    "    wordLang1: str, 'perro'\n",
    "    set2: dict or list, ['hello','dog']\n",
    "    sourceLang: str, 'es'\n",
    "    targetLang: str, 'en'\n",
    "    return list\n",
    "    \"\"\"\n",
    "    global model\n",
    "    global transmat\n",
    "    d = []\n",
    "    vec1 = model[sourceLang].get_sentence_vector(wordLang1)\n",
    "    for s2 in setLang2:\n",
    "        vec2= model[targetLang].get_sentence_vector(s2.strip().replace('_',' '))\n",
    "        vec2T = apply_transform(vec2,transmat[sourceLang][targetLang])\n",
    "        dist = distance.cosine(vec1,vec2T)\n",
    "        d.append((dist,s2))\n",
    "    return sorted(d)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for the most similar word to: cat\n",
      "list ['gato', 'automóvil', 'perro', 'camión', 'geografía', 'montañas', 'rios', 'baloncesto', 'futbol']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.43776026412466407, 'gato')"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordEn ='cat'\n",
    "print('Searching for the most similar word to:', wordEn)\n",
    "print('list',words['es'])\n",
    "getMoreSimilar(wordEn,words['es'],'en','es')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for the most similar word: kitty\n",
      "list ['gato', 'automóvil', 'perro', 'camión', 'geografía', 'montañas', 'rios', 'baloncesto', 'futbol']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.647667168023605, 'gato')"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordEn ='kitty'\n",
    "print('Searching for the most similar word:', wordEn)\n",
    "print('list',words['es'])\n",
    "getMoreSimilar(wordEn,words['es'],'en','es')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aligning set of words\n",
    "\n",
    "Given two sets of words, get a mapping one-to-one mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-to-one mappings\n",
    "def alignSets(set1,set2,sourceLang,targetLang,sensivity=.45):\n",
    "    \"\"\"\n",
    "    Given two sets of words/sentences in two languages\n",
    "    return the possible alignments between sentences\n",
    "    set1: dict or list, ['hola','perro']\n",
    "    set2: dict or list, ['hello','dog']\n",
    "    sourceLang: str, 'es'\n",
    "    targetLang: str, 'en'\n",
    "    return list\n",
    "    \"\"\"\n",
    "    global model\n",
    "    global transmat\n",
    "    output = []\n",
    "    G= nx.Graph()\n",
    "    for s1 in set1:\n",
    "        vec1 = model[sourceLang].get_sentence_vector(s1.strip().replace('_',' '))\n",
    "        for s2 in set2:\n",
    "                    vec2= model[targetLang].get_sentence_vector(s2.strip().replace('_',' '))\n",
    "                    vec2T = apply_transform(vec2,transmat[sourceLang][targetLang])\n",
    "                    dist = distance.cosine(vec1,vec2T)\n",
    "                    if dist < sensivity:\n",
    "                        node1= '%s_%s' % (sourceLang,s1)\n",
    "                        node2= '%s_%s' % (targetLang,s2)\n",
    "                        G.add_edge(node1,node2)\n",
    "                        G[node1][node2]['w'] = dist\n",
    "\n",
    "                \n",
    "    while G.edges():\n",
    "            p = sorted(G.edges(data=True), key=lambda x: x[2]['w'])[0]\n",
    "            psorted = sorted(list(p[:2]))\n",
    "            output.append({psorted[0][:2]:psorted[0][3:],psorted[1][:2]:psorted[1][3:],'d':p[2]['w']})\n",
    "            G.remove_node(p[0])\n",
    "            G.remove_node(p[1])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cat', 'kitty', 'motocycle', 'car', 'dog', 'truck', 'geography', 'mountains', 'rivers', 'basketball', 'football']\n",
      "['gato', 'automóvil', 'perro', 'camión', 'geografía', 'montañas', 'rios', 'baloncesto', 'futbol']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'d': 0.20396928012967108, 'en': 'basketball', 'es': 'baloncesto'},\n",
       " {'d': 0.2424814900096134, 'en': 'mountains', 'es': 'montañas'},\n",
       " {'d': 0.27150193662130007, 'en': 'truck', 'es': 'camión'},\n",
       " {'d': 0.2744324328139399, 'en': 'car', 'es': 'automóvil'},\n",
       " {'d': 0.2938059497380646, 'en': 'geography', 'es': 'geografía'},\n",
       " {'d': 0.29473989913984433, 'en': 'dog', 'es': 'perro'},\n",
       " {'d': 0.3924369271143857, 'en': 'football', 'es': 'futbol'},\n",
       " {'d': 0.43776026412466407, 'en': 'cat', 'es': 'gato'}]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(words[lang1])\n",
    "print(words[lang2])\n",
    "\n",
    "alignSets(words[lang1],words[lang2],lang1,lang2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
